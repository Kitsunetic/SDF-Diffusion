exp_dir: results/sr32_64
epochs: 1000
seed: 0
memo: 

model:
  target: src.models.unet_sr3.UNet
  params:
    dims: 3
    in_channel: 2
    out_channel: 1
    inner_channel: 64
    norm_groups: 32
    channel_mults: [1, 2, 4]
    attn_res: [8]
    res_blocks: 4
    dropout: 0.1
    with_noise_level_emb: yes
    use_affine_level: yes
    image_size: 32
    num_classes: null
    additive_class_emb: yes
    use_nd_dropout: no

ddpm:
  train: &diffusion
    target: src.models.diffusion.GaussianDiffusion
    params:
      loss_type: l1
      model_mean_type: x_0
      schedule_kwargs:
        schedule: linear
        n_timestep: 1000
        linear_start: 1.e-4
        linear_end: 2.e-2
        ddim_S: 50
        ddim_eta: 0.0
  valid: *diffusion

preprocessor:
  target: src.models.trainers.sr3d.SR3dPreprocessor
  params:
    do_augmentation: yes
    sdf_clip: [0.0625, 0.03125]
    mean: [0.0, 0.0]
    std: [0.0625, 0.03125]
    patch_size: 32
    downsample: 1

trainer:
  target: src.models.trainers.sr3d.SR3dTrainer
  params:
    find_unused_parameters: no
    sample_at_least_per_epochs: 20
    mixed_precision: yes
    n_samples_per_class: 12
    n_rows: 6
    use_ddim: yes
    ema_decay: 0.99

dataset:
  target: src.datasets.dataset_sr.build_dataloaders
  params:
    ds_kwargs:
      datafile_lr: ../data/sdf.res32.level0.0500.PC15000.pad0.20.hdf5
      datafile_hr: ../data/sdf.res64.level0.0313.PC15000.pad0.20.hdf5
      cates: chair
    dl_kwargs:
      batch_size: 32
      num_workers: 8
      pin_memory: yes
      persistent_workers: yes

optim:
  target: torch.optim.Adam
  params:
    lr: 0.0001
    weight_decay: 0.0

train:
  clip_grad: 1.0
  num_saves: 10

criterion:
  target: torch.nn.MSELoss

sched:
  target: src.scheduler.ReduceLROnPlateauWithWarmup
  params:
    mode: min
    factor: 0.9
    patience: 10
    verbose: yes
    threshold: 1.e-8
    min_lr: 1.e-5
    warmup_steps: 1
  step_on_batch: no
  step_on_epoch: yes

sample:
  epochs_to_save: 9
